# Long-Document QA with Chain-of-Structured-Thought and Fine-Tuned SLMs
> A two-stage RL-enhanced framework that equips SLMs for high-accuracy long-document QA.

<div align="center">

[![OpenReview](https://img.shields.io/badge/OpenReview-LiteCoST-brightgreen.svg)](https://openreview.net/forum?id=faECRsdRav)
[![Model](https://img.shields.io/badge/ğŸ¤—-Data-orange)](https://github.com/SetonLiang/LiteCoST)
[![Python](https://img.shields.io/badge/Python-3.11.10-3776AB.svg?style=flat)](https://www.python.org/downloads/release/python-31110/)

</div>


## ğŸ‰ **News**
- **[2026-01-26]** Our **[LiteCoST](https://github.com/SetonLiang/LiteCoST)** is accepted by **ICLRâ€™26**.


## ğŸ“‹ **Overview**

<div align="center">
  <img src="assets/framework.png" alt="Overview Figure" width="600"/>
</div>

**Pillar 1: Chain-of-Structured-Thought (CoST)** uses a high-capability LLM purely as a trace generator: it proposes a minimal structure, executes a step-wise, structure-guided trace over the documents, serializes the result, and verifies/refines it (optionally with an LLM-as-judge). 

<div align="center">
  <img src="assets/grpo.png" alt="Overview Figure" width="600"/>
</div>

**Pillar 2: SLM fine- tuning (SFT â†’ GRPO)** trains an SLM with the CoST supervision in two phases: Supervised Fine-Tuning to learn structural patterns, formatting rules, and reasoning steps, followed by Group Relative Policy Optimization with dual signals that reward both answer/format quality and step/process consistencyâ€”transferring structure-first behavior to an efficient SLM for low-latency deployment.

## ğŸ—ï¸ Method & Architecture
### CoST: Structure-First Reasoning and Trace Generation 
1. ğŸ” Structure Analysis
2. ğŸ§  Trace Geneartion
3. âœ… Data Verification
4. ğŸ” Data Refinement


### SLM Fine-Tuning: SFT â†’ GRPO
1. ğŸ¯ Supervised Fine-Tuning (SFT)
2. âš¡ Group Relative Poilcy Optimization (GRPO)

The core execution of **LiteCoST** is implemented in the src directory (See GRPO in *'verl/'*):
```text
src
â”œâ”€â”€ convert_func.py              # Conversion function module
â”œâ”€â”€ data_refinement.py           # Data refinement module
â”œâ”€â”€ data_verification.py         # Data verification module
â”œâ”€â”€ extract/                     # Extraction module
â”‚   â”œâ”€â”€ graph.py                 # Graph class
â”‚   â”œâ”€â”€ main.py                  # Main program
â”‚   â”œâ”€â”€ table.py                 # Table class
â”‚   â”œâ”€â”€ to_desc.py               # Convert to description
â”‚   â”œâ”€â”€ to_graph.py              # Convert to graph 
â”‚   â””â”€â”€ to_table.py              # Convert to table
â”œâ”€â”€ sft.py                       # SFT module
â”œâ”€â”€ prompt.py                    # Prompt template module
â”œâ”€â”€ reasoner.py                  # Reasoning module
â”œâ”€â”€ reward.py                    # Reward module
â”œâ”€â”€ structure_analysis/          # Structure analysis module
â”‚   â”œâ”€â”€ query2schema.py          # Schema construction
â”‚   â””â”€â”€ structure_decision.py    # Structure decision
â”œâ”€â”€ cal_latenct.py               # Calculate Latency
â””â”€â”€ utils.py                     # Utility functions module
```

## ğŸ› ï¸ **Usage**
1. Generate the Serialized Structured Output 
```python
python main.py --model gpt-4o --dataset Loong --structured --document

cd src
python data_verification.py
python data_refinement.py
```

2. Conduct SFT Training
```python
python -m src.convert_func # data format conversion
python -m src.sft
```

3. Conduct GRPO Optimization
```python
cd verl
bash scripts/run_grpo_cost.sh

## merge model 
python scripts/model_merger.py merge --backend fsdp --local_dir checkpoints/cost-sft/cost-sft-llama3.2-3b-ins/global_step_1566/actor --target_dir merged/cost-grpo/llama3.2-3b-ins
```

## **Usage Examples**
```python
1. Quick Deployment
cd Loong/src
bash vllm_example.sh

2. Run the pipeline
python main.py --model deployed_model --dataset Loong --structured --document
```


## ğŸ¯ **Performance**

<div align="center">

<p><b>Efficacy of Chain-of-Structured-Thought (CoST).</b></p>

<div style="display: flex; justify-content: center; align-items: flex-start; gap: 16px;">
  <figure style="margin: 0; flex: 1; text-align: center;">
    <img src="assets/CoST_finance.png" alt="Finance" style="width: 100%; height: auto; max-height: 320px; object-fit: contain;">
    <figcaption><b>Finance</b></figcaption>
  </figure>
  <!-- <figure style="margin: 0; flex: 1; text-align: center;">
    <img src="assets/CoST_general.png" alt="General Knowledge" style="width: 100%; height: auto; max-height: 320px; object-fit: contain;">
    <figcaption><b>General Knowledge</b></figcaption> -->
  </figure>
</div>

<p><b>Effectiveness: How good is LiteCoST for SSO Generation?</b></p>

<div style="display: flex; justify-content: center; gap: 16px; margin-top: 16px;">
  <figure style="margin: 0; flex: 1;">
    <img src="assets/litecost_finance.png" alt="Finance" style="width: 100%; height: auto;">
    <figcaption><b>Finance</b></figcaption>
  </figure>
  <figure style="margin: 0; flex: 1;">
    <img src="assets/litecost_legal.png" alt="Legal" style="width: 100%; height: auto;">
    <figcaption><b>Legal</b></figcaption>
  </figure>
  <!-- <figure style="margin: 0; flex: 1;">
    <img src="assets/litecost_general.png" alt="General Knowledge" style="width: 100%; height: auto;">
    <figcaption><b>General Knowledge</b></figcaption>
  </figure> -->
</div>

</div>

## Acknowledgement
We implement our reinforcement learning algorithm by extending the [veRL framework](https://github.com/volcengine/verl). For efficient inference, we leverage [vLLM](https://github.com/vllm-project/vllm), and we develop evaluation scripts based on the [Loong datasets](https://github.com/MozerWang/Loong). We sincerely thank these communities for their valuable contributions!


